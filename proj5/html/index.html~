<html>
<head>
<title>Boundary Detection with Sketch Tokens</title>
<link href='http://fonts.googleapis.com/css?family=Nunito:300|Crimson+Text|Droid+Sans+Mono' rel='stylesheet' type='text/css'>
<link rel="stylesheet" title="Default" href="styles/github.css">
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>  

<link rel="stylesheet" href="highlighting/styles/default.css">
<script src="highlighting/highlight.pack.js"></script>

<style type="text/css">
body {
	margin: 0px;
	width: 100%;
	font-family: 'Crimson Text', serif;
	font-size: 20px;
	background: #fcfcfc;
}
h1 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 28px;
	margin: 25px 0px 0px 0px;
	text-transform: lowercase;

}

h2 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 32px;
	margin: 15px 0px 35px 0px;
	color: #333;	
	word-spacing: 3px;
}

h3 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 26px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}
h4 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 22px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}

h5 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 18px;
	margin: 10px 0px 10px 0px;
	color: #111;
	word-spacing: 2px;
}

p, li {
	color: #444;
}

a {
	color: #DE3737;
}

.container {
	margin: 0px auto 0px auto;
	width: 1160px;
}

#header {
	background: #333;
	width: 100%;
}

#headersub {
	color: #ccc;
	width: 960px;
	margin: 0px auto 0px auto;
	padding: 20px 0px 20px 0px;
}

.chart {
	width: 480px;
}
.lol {
	font-size: 16px;
	color: #888;
	font-style: italic;
}
.sep {
	height: 1px;
	width: 100%;
	background: #999;
	margin: 20px 0px 20px 0px;
}
.footer{
	font-size: 16px;
}
.latex {
	width: 100%;
}

.latex img {
	display: block;
	margin: 0px auto 0px auto;
}

pre {
	font-family: 'Droid Sans Mono';
	font-size: 14px;
}

table td {
  text-align: center;
  vertical-align: middle;
}

table td img {
  text-align: center;
  vertical-align: middle;
}

#contents a {
}
</style>
<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>
</head>
<body>
<div id="header" >
<div id="headersub">
<h1>Sarah Parker <span style="color: #DE3737">(smparker)</span></h1>
</div>
</div>
<div class="container">

<h2>CS 143 / Project 5 / Boundary Detection with Sketch Tokens</h2>


<p> 	Boundary detection is a commonly studied problem in computer vision and several algorithms have been developed in order to address this problem. In project 5, we implemented the recently published Sketch Tokens boundary detector pipeline. This pipeline relies only on local information and uses simple gradient and color features. We used the Berkeley segmentation dataset which contains 500 images (200 training, 100 validation, 200 test.) Each of the images corresponds to at least 4 human segmentations.  </p>

<div style="clear:both">
<h3>Implementation</h3>

<p> 	Training images were converted into 14 channels: 3 LUV color channels, 3 overally gradient magnitude channels and 8 oriented gradient magnitude channels. In order to compute the oriented gradient magnitude channels, I created 4 sobel-like filters at different orientations (0, pi/4, pi/2, 3pi/4.) I also added self similarity features which are defined by splitting an image patch of channel features into an mxm grid and summing all of the values in the patch. Self similarity is measured by finding the difference between all unique pairs of patch sums. At training time, positive and negative examples of contours are extracted from the training set using the corresponding human annotation images. Positive examples are 15x15 patches centered on an annotation boundary. Human annotations were converted into DAISY features and patches from the same location as each of the previously extracted positive examples are extracted from these features. The DAISY patches are clustered using k-means and the cluster labels are transferred over to the postive image examples to be used as classification labels. Negative image features are simply labeled with a 1. A random forest classifier is trained using these labels and image features. At testing time, test images are padded using reflected image content and converted into channels. 15x15 patches are extracted around each pixel, self similarity features are computed, and the resulting features are classified using the previously trained random forest. A contour-probability map is computed for each test image. The map is blurred using a gaussian kernel and non-maximum suppression is performed. Resulting contour images and result statistics are shown below. </p>

<h2>Results</h2>

<h3>Input Images</h3>

<table border=1>
<tr>
<td>
<img src="3063.jpg" width="24%"/>
<img src="35049.jpg"  width="24%"/>
<img src="49024.jpg" width="24%"/>
<img src="70011.jpg" width="24%"/>
</td>
</tr>

<tr>
<td>
<img src="35028.jpg" width="24%"/>
<img src="43051.jpg"  width="24%"/>
<img src="196027.jpg" width="24%"/>
<img src="106005.jpg" width="24%"/>
</td>
</tr>

</table>


<p>


<h4>Contour Images Produced</h4>

<table border=1>
<tr>
<td>
<img src="3063.png" width="24%"/>
<img src="35049.png"  width="24%"/>
<img src="49024.png" width="24%"/>
<img src="70011.png" width="24%"/>
</td>
</tr>

<tr>
<td>
<img src="35028.png" width="24%"/>
<img src="43051.png"  width="24%"/>
<img src="196027.png" width="24%"/>
<img src="106005.png" width="24%"/>
</td>
</tr>

</table>



<center>
</br>
</br>

<h5>PR Curve</h5>
<img src="PR_curve.png">
<p> F score of 0.65 using self similarity </p>

<p> F score of 0.63 without self similarity </p>

</center>

<div style="clear:both" >
<p> 	</p>
</div>
</body>
</html>
